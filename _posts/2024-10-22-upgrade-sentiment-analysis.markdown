---
layout: post
title:  "감성분석 업그레이드"
date:   2024-10-22 00:00:00 +0900
author: Jhk
---
안녕하세요. 텍스톰에서 NLP를 연구하고 있는 데이터사이언스연구소의 강지훈 주임연구원입니다.

오늘은 감성분석을 업그레이드하게 된 이유와 새로 도입한 모델과의 성능 비교 그리고 향후 발전 방향에 대해 설명드리겠습니다.

# 1. Naive Bayes 알고리즘

기존 텍스톰에서는 Naive Bayes 알고리즘을 사용한 문서기반 감성분석을 제공하고 있었습니다. Naive Bayes는 사전확률에 기반하여 사후확률을 추론하는 알고리즘입니다.
감성분석을 예로 들면, 사전확률은 각 단어들이 감정(긍정/부정/중립)에 속할 확률이고 사후확률은 주어진 문서를 구성하는 각 단어들의 감정 확률로 계산한 문서의 감정 확률입니다.
여기서 각 단어들이 긍정/부정/중립에 속할 확률은 유저가 제공한 학습 데이터를 바탕으로 계산됩니다. 

### 1.1 Naive Bayes 알고리즘의 작동 방식
Naive Bayes는 간단하게 감정의 발생 확률 P(Sentiment)과 감정 내 단어들이 발생할 확률 P(Word|Sentiment)의 곱으로 계산됩니다.
여기서 감정의 발생 확률 P(S)은 학습한 데이터의 감성 분포를 사용합니다. 예를 들면, 학습에 사용한 데이터가 100개이고 그 중에서 긍정으로 라벨링 된 문서가 50개, 부정으로 라벨링 된 문서가 30개, 중립으로 라벨링된 문서가 20개라면 긍정의 발생 확률은 0.5, 부정의 발생 확률은 0.3 중립의 발생 확률은 0.2로 계산할 수 있습니다.
감정 내 단어들이 발생할 확률 P(Word|Sentiment)은 특정 감정을 가진 모든 문서에 포함된 단어들의 빈도수 중에서 특정 단어의 빈도수로 비율을 계산합니다. 예를 들어 긍정으로 라벨링 된 문서 50개 안에 포함된 모든 단어를 세었을 때 500개이고 그 중에서 '재미있다'라는 단어가 50번 나타났으면, P(재미있다|긍정)은 0.1로 계산할 수 있습니다.
그리고 새로운 문서의 감정을 분류할 때는 각 감정별로 확률값을 계산한 후, 최대 확률값을 나타내는 감정을 해당 문서의 감정으로 분류합니다.

### 1.2 장점
이처럼 단어별 감성분포를 계산할 때 빈도수만 사용하기 때문에 학습 과정이 간단하고 빠릅니다. 그리고 새로운 문장의 감성을 분류할 때도 문장에 속한 단어들의 확률을 곱하여 단순하게 계산이 가능하기 때문에 속도가 매우 빠르다는 장점이 있습니다. 
또한 성능면에서도 충분한 데이터로 학습했을 때 준수한 성능을 보입니다. 그래서 분류 문제에 일반적으로 많이 사용 되었습니다.

### 1.3 단점
하지만 단순히 빈도만 사용하기 때문에 성능에 한계를 가질 수 밖에 없습니다. 왜냐하면 동음이의어나 단어의 순서와 같은 문맥이 고려되지 않기 때문입니다. 예를 들어, Naive Bayes로 계산했을 때는 '아이폰 프로는 사이즈가 너무 <U>커서 안 좋아요</U>'라는 문장과 '아이폰 미니는 사이즈가 <U>안 커서 좋아요</U>'라는 문장에서 좋다와 안 좋다는 감성의 차이가 명확하지만 순서가 고려되지 않기 때문에 확률을 계산할 때는 차이가 없습니다.
결정적으로 Naive Bayes는 학습에 제공된 데이터'만'으로 모델이 구성되기 때문에 처음 본 단어가 있을 경우, 확률이 제대로 계산되지 않기 때문에 성능이 크게 떨어지게 됩니다. 그렇기 때문에 Naive Bayes 알고리즘은 '정확하게' 정답을 라벨링한 많은 종류의 데이터가 필요합니다.

이런 Naive Bayes의 단점을 극복하기 위해서 신경망 기반의 언어모델인 ELECTRA와 GPT를 도입하게 되었습니다.

# 2. 신경망 언어모델

신경망 언어모델은 언어의 복잡한 패턴을 처리할 수 있는 Transformer 구조와 사전훈련 모델의 장점으로 Naive Bayes가 가진 단점을 극복할 수 있습니다.

### 2.1 Transformer 구조의 장점
Transformer 구조의 가장 큰 특징은 문맥을 이해하는 방식에 있습니다.

Naive Bayes는 각 단어의 출현 빈도나 확률을 기반으로 동작합니다. 문서를 벡터로 표현할 때 주로 BoW(Bag of Words) 방식을 사용하는데, 이는 단어의 순서를 무시하고 각 단어의 출현 빈도만을 고려합니다. 예를 들어 "음식이 맛있다"라는 문장은 {'음식':1, '맛있다':1}와 같이 표현됩니다. Naive Bayes에서는 이러한 빈도를 바탕으로 각 단어가 특정 감성에서 등장할 조건부 확률을 계산합니다. 이러한 방식은 계산이 단순하고 효율적이지만, 단어 간의 의미적 관계나 문맥을 전혀 고려하지 못한다는 한계가 있습니다.

반면 Transformer는 각 단어를 수백 차원의 벡터로 표현하여, 단어의 의미를 더욱 풍부하게 담아낼 수 있습니다. 예를 들어 '좋다'와 '훌륭하다'는 의미상 유사하기 때문에 비슷한 벡터값을 가지게 됩니다. 이를 통해 각 단어가 더 많은 의미를 함축할 수 있게 됩니다.

최근의 Transformer에 기반한 언어모델들은 attention 구조를 가지고 있습니다. attention은 문장 내 각 단어들 간의 관계를 계산하여 문맥을 이해합니다. 예를 들어 "음식은 맛있는데 서비스가 아쉽다"라는 문장에서, '맛있는'이라는 단어는 '음식'과 강하게 연관되어 있고, '아쉽다'는 '서비스'와 강하게 연관되어 있다는 것을 파악할 수 있습니다. 이러한 구조 덕분에 문장의 전후관계를 정확하게 이해하고, 특정 단어가 어떤 대상을 수식하는지 파악할 수 있습니다.

이처럼 Transformer는 단어의 의미를 더 풍부하게 표현하고 문장의 문맥을 정확하게 이해할 수 있는 구조를 가지고 있어, 기존의 Naive Bayes가 가진 한계를 극복할 수 있습니다.

### 2.2 사전훈련 모델의 장점
ELECTRA의 기본 모델(base model)은 위키피디아와 책에서 수집한 33억개의 토큰으로 사전학습을 시켰으며 대형 모델(large-model)은 웹에서 수집한 데이터를 추가하여 330억개의 토큰으로 사전 학습시켰습니다. 330억개의 토큰은 A4용지 기준으로 500만 페이지가 되며 300장 분량의 전공서적, 약 1만 7천권에 해당하는 방대한 양의 데이터입니다.

그리고 google에서 개발한 ELECTRA는 주로 영어 데이터로 학습되었기 때문에 한국어와 중국어 모델은 기본 ELECTRA 모델에 각 언어로 구성된 데이터로 추가 학습을 시킨 모델입니다. 텍스톰에서 제공하는 한국어 모델(electra-kor-base)은 [김기영](https://github.com/kiyoungkim1) 님이 뉴스, 블로그, 댓글, 리뷰 등으로 구성된 70GB의 한국어 데이터셋으로 학습시킨 모델을 사용했습니다.

즉, 100개 ~ 1,000개의 라벨링된 학습데이터로 모델을 파인튜닝 했을 때, 모델은 100개 ~ 1,000개의 데이터만 학습한 것이 아니라 사전훈련된 방대한 양의 데이터를 기반으로 학습하는 것입니다. 이러한 사전훈련 모델의 장점으로 모델은 기본적인 감성분석 능력을 갖추고 있으며 도메인 이해에 필요한 적은 수의 데이터만으로 높은 성능을 낼 수 있습니다.

# 3. 성능 비교

그렇다면 위에 설명 드린 3가지 알고리즘을 감성분석에 적용했을 때 어느 정도로 성능에 차이를 보이는지 실험을 통해 알아보겠습니다.

### 3.1 데이터셋 구성
먼저 데이터셋은 박은정님이 공개하신 [영화 리뷰 데이터](https://github.com/e9t/nsmc)를 사용하였습니다. 이 데이터셋은 네이버 영화에서 리뷰와 함께 평점을 수집하여 1~4 점은 부정, 9~10점은 긍정 라벨링한 데이터입니다. 그리고 긍정과 부정의 비율이 5대5가 되도록 전체 64만 개의 리뷰에서  20만 개를 샘플링하였습니다.
다운로드 한 데이터에서 URL만 있거나 특수문자만 있는 데이터를 제거하고 전처리가 완료된 데이터셋을 무작위로 분할하여 학습용 데이터 60%, 테스트용 데이터 40%로 구성하였습니다.

### 3.2 모델 학습
Naive Bayes도 데이터의 형태나 분포의 가정에서 따라서 알고리즘이 세부적으로 구분되며, tokenizer에 따라서도 성능이 달라집니다. 테스트에 사용할 각 알고리즘의 세부 스팩은 다음과 같습니다.
- Naive Bayes : 텍스트 분류에 주로 사용되는 multinomial naive bayes를 사용하였으며, tokenizer로는 wordpiece tokenier를 사용하였습니다.
- Electra : 구글에서 개발한 기본형 모델([google/electra-base-discriminator](https://huggingface.co/google/electra-base-discriminator))에 한국어 corpus로 학습시킨 모델([kykim/electra-kor-base](https://huggingface.co/kykim/electra-kor-base))을 사용하였습니다.
- GPT : OpenAI에서 제공하는 API로 gpt-4o-mini 모델을 사용하였습니다. gpt-4o의 성능이 더 좋겠지만 빅데이터 분석 서비스를 제공하고 있기 때문에 서비스에 적용이 가능하도록 비용을 고려하여 mini 모델로 테스트하였습니다.
Naive Bayes와 ELECTRA는 구성된 데이터로 학습을 시켰으며, GPT는 범용적인 성능을 테스트 하기 위해서 zero-shot을 적용하였습니다.

### 3.3 테스트 결과

|               | Naive Bayes | ELECTRA<br>(electra-kor-base) | GPT<br>(gpt-4o-mini) |
| :-----------: | :---------: | :---------------------------: | :------------------: |
| **accuracy**  |   0.7748    |          **0.8560**           |        0.8476        |
| **precision** |   0.7753    |          **0.8573**           |        0.8571        |
|  **recall**   |   0.7748    |          **0.8560**           |        0.8476        |
| **f1 score**  |   0.7747    |          **0.8558**           |        0.8466        |

Naive Bayes는 12만 개의 데이터를 학습했음에도 동일한 데이터를 사용한 ELECTRA에 비해서 8%p나 성능이 떨어졌고 아무런 데이터를 추가 학습하지 않은 GPT에 비해서도 7%p가 떨어졌습니다. 이런 점은 문맥을 제대로 고려하지 못하는 통계적 방법론의 한계로 생각할 수 있습니다.

하지만 한 가지 의아하게 생각할 수 있습니다. 뛰어난 성능으로 모두를 놀라게 만드는 GPT가 ELECTRA보다 성능이 미세하지만 낮기 때문입니다. 그 이유는 ELECTRA는 12만 개의 데이터로 추가 학습 하면서 영화 리뷰의 문맥을 잘 이해할 수 있었기 때문입니다. 반대로 생각하면 GPT는 어떠한 추가 학습 없이도 많은 데이터를 추가 학습한 ELECTRA와 1%p 차이로 거의 비슷한 성능을 보인 것입니다.

텍스톰이라는 서비스를 운영하는 입장에서는 유저들이 일일이 라벨링 하여 데이터를 올리기는 어렵기 때문에 라벨링 없이도 높은 성능을 보이는 GPT와 유저가 업로드한 데이터로 학습하여 GPT 대비 더 높은 성능을 보이는 ELECTRA 2가지 모델을 추가하게 되었습니다.

# 4. 향후 발전 방향

처음에는 감성분석의 성능을 높이는 연구로 시작했지만 실제 서비스에 적용하는 과정에서 개선해야 할 점이 많이 보였습니다.

### 4.1. 속성기반 감성분석의 필요성
문서기반 감성분석은 문서 전체를 긍정/부정/중립으로 분류하기 때문에 하나의 문서 안에 여러 감정이 포함되어 있는 경우 이를 정확하게 판별하기 어렵습니다. 예를 들어서, 음식점의 리뷰를 분석 했을 때 맛과 분위기는 긍정적으로 생각하지만 동시에 서비스와 위생은 부정적으로 생각할 수 있습니다. 각 요소별로 보면 긍정과 부정이 명확하지만 전체 리뷰로 보면 긍정인지 부정인지 사람이 보아도 명확하게 말할 수 없을 것입니다.
아무리 GPT와 같이 뛰어난 모델을 사용한다 해도 이처럼 문서기반 감성분석 자체가 가지는 한계 때문에 분석이 잘못되었다고 느끼는 경우가 있습니다. 따라서 문서가 아니라 문장, 나아가 속성 단위로 감성분석을 해야지 더 정확하게 분석할 수 있습니다.

### 4.2. 학습 데이터 수 최적화
이번 실험에서 GPT의 성능이 ELECTRA에 비해 낮게 나온 것도 어떤 학습 데이터도 제공하지 않았기 때문입니다. 즉, 정확한 감성분석을 위해서는 분석 도메인에 맞는 데이터로 모델을 추가 학습 시켜야 합니다. 이번 실험에서 12만 개의 데이터로 학습한 ELECTRA가 GPT(zero-shot)보다 높은 성능을 보인 것처럼 높은 성능을 위해서는 많은 데이터가 필요합니다.
하지만 이런 데이터를 일일이 라벨링 하는 것은 많은 노력이 필요하고 유저 경험을 저해합니다. 그렇기 때문에 더 적은 데이터만 학습해도 높은 성능을 끌어낼 수 있는 방법이 필요합니다.
그래서 저희는 100개의 데이터로 학습해도 10,000개의 데이터로 학습한 모델과 비슷한 성능을 유지할 수 있는 방안을 연구하고자 합니다.